---
title: "SCPME"
output: github_document
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[![Build Status](https://travis-ci.org/MGallow/shrink.svg?branch=master)](https://travis-ci.org/MGallow/SCPME)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/SCPME)](https://cran.r-project.org/package=SCPME)

## Overview

`shrink` is an implementation of the methods described in "Shrinking Characteristics of Precision Matrix Estimators" [pdf](https://doi.org/10.1093/biomet/asy023). It estimates a penalized precision matrix via a modified alternating direction method of multipliers (ADMM) algorithm.

<p align="center">
  <img src = "https://github.com/MGallow/SCPME/raw/master/vignettes/images/gif.gif"/>
</p>

A (possibly incomplete) list of functions contained in the package can be found below:

* `shrink()` computes the estimated precision matrix

* `plot.shrink()` produces a heat map or line graph for cross validation errors

See [vignette](https://mgallow.github.io/SCPME/) or [manual](https://github.com/MGallow/ADMMsigma/blob/master/SCPME.pdf).

## Installation

```{r, eval = FALSE}
# The easiest way to install is from GitHub:
# install.packages("devtools")
devtools::install_github("MGallow/shrink")
```

If there are any issues/bugs, please let me know: [github](https://github.com/MGallow/SCPME/issues). You can also contact me via my [website](https://mgallow.github.io/). Pull requests are welcome!

## Usage

```{r, message = FALSE}
library(SCPME)
set.seed(123)

# let's generate some data!

# specify marginal covariance of X
# note that the inverse is tri-diagonal (sparse)
Sxx = matrix(0.7, nrow = 5, ncol = 5)
for (i in 1:5){
  for (j in 1:5){
    Sxx[i, j] = Sxx[i, j]^abs(i - j)
  }
}

# now randomly generate some 100 observations of X
Z = matrix(rnorm(100*5), nrow = 100, ncol = 5)
out = eigen(Sxx, symmetric = TRUE)
Sxx.sqrt = out$vectors %*% diag(out$values^0.5) %*% t(out$vectors)
X = Z %*% Sxx.sqrt

# randomly generate regression coefficients
betas = matrix(rnorm(5, 0, sqrt(1/5)), nrow = 5, ncol = 1)

# we will also assume a sparse matrix here
betas = betas*matrix(rbinom(5, 1, prob = 0.5), nrow = 5, ncol = 1)

# now we randomly generate Y
Y = X %*% betas + rnorm(100)



# print marginal sample precision matrix for X
# this is perhaps a bad estimate (not sparse)
Sample = (nrow(X) - 1)/nrow(X)*cov(X)
round(qr.solve(Sample), 5)

# estimate preicison matrix (omega) assuming sparsity
# note that this is simply lasso penalized preicision matrix
shrink(X, lam = 0.5)

# what if we instead assumed sparsity in beta?
# recall that beta is a product of marginal precision of X and cov(X, Y)
shrink(X, Y, B = cov(X, Y), nlam = 20, lam.max = max(abs(t(X) %*% Y)))

# we could also assume sparsity in beta AND omega
shrink(X, Y, B = cbind(cov(X, Y), diag(ncol(X))), nlam = 20, lam.max = 10, lam.min.ratio = 1e-4)

# produce CV heat map for shrink
(shrink = shrink(X, Y, B = cov(X, Y), nlam = 20, lam.max = max(abs(t(X) %*% Y))))
plot(shrink, type = "heatmap")

# produce line graph for CV errors for shrink
plot(shrink, type = "line")

```

<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>SCPME Algorithm Details • SCPME</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="SCPME Algorithm Details">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-84680750-8"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-84680750-8');
</script>
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">SCPME</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Tutorial.html">Tutorial</a>
    </li>
    <li>
      <a href="../articles/Details.html">Details</a>
    </li>
    <li>
      <a href="../articles/Benchmark.html">Benchmark</a>
    </li>
    <li>
      <a href="../reference/index.html">Functions</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Related Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="https://mgallow.github.io/ADMMsigma/">ADMMsigma</a>
    </li>
    <li>
      <a href="https://mgallow.github.io/GLASSOO/">GLASSOO</a>
    </li>
    <li>
      <a href="https://mgallow.github.io/CVglasso/">CVglasso</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="../news/index.html">
    <span class="fa fa-newspaper-o fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/MGallow/SCPME/">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://mgallow.github.io/">
    <span class="fa fa-user fa-lg"></span>
     
    MGallow
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>SCPME Algorithm Details</h1>
                        <h4 class="author">Matt Galloway</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/MGallow/SCPME/blob/master/vignettes/Details.Rmd"><code>vignettes/Details.Rmd</code></a></small>
      <div class="hidden name"><code>Details.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>Consider the case where we observe <span class="math inline">\(n\)</span> independent, identically distributed copies of the random variable (<span class="math inline">\(X_{i}\)</span>) where <span class="math inline">\(X_{i} \in \mathbb{R}^{p}\)</span> is normally distributed with some mean, <span class="math inline">\(\mu\)</span>, and some variance, <span class="math inline">\(\Sigma\)</span>. That is, <span class="math inline">\(X_{i} \sim N_{p}\left( \mu, \Sigma \right)\)</span>.</p>
<p>Because we assume independence, we know that the probability of observing these specific observations <span class="math inline">\(X_{1}, ..., X_{n}\)</span> is equal to</p>
<p><span class="math display">\[\begin{align*}
  f(X_{1}, ..., X_{n}; \mu, \Sigma) &amp;= \prod_{i = 1}^{n}(2\pi)^{-p/2}\left| \Sigma \right|^{-1/2}\exp\left[ -\frac{1}{2}\left( X_{i} - \mu \right)^{T}\Sigma^{-1}\left( X_{i} - \mu \right) \right] \\
  &amp;= (2\pi)^{-nr/2}\left| \Sigma \right|^{-n/2}\mbox{etr}\left[ -\frac{1}{2}\sum_{i = 1}^{n}\left( X_{i} - \mu \right)\left( X_{i} - \mu \right)^{T}\Sigma^{-1} \right]
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mbox{etr}\left( \cdot \right)\)</span> denotes the exponential trace operator. It follows that the log-likelihood for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> is equal to the following:</p>
<p><span class="math display">\[ l(\mu, \Sigma | X) = const. - \frac{n}{2}\log\left| \Sigma \right| - tr\left[ \frac{1}{2}\sum_{i = 1}^{n}\left(X_{i} - \mu \right)\left(X_{i} - \mu \right)^{T}\Sigma^{-1} \right] \]</span></p>
<p>If we are interested in estimating <span class="math inline">\(\mu\)</span>, it is relatively straight forward to show that the maximum likelihood estimator (MLE) for <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\hat{\mu}_{MLE} = \sum_{i = 1}^{n}X_{i}/n\)</span> which we typically denote as <span class="math inline">\(\bar{X}\)</span>. However, in addition to <span class="math inline">\(\mu\)</span>, many applications require the estimation of <span class="math inline">\(\Sigma\)</span> as well. We can also find a maximum likelihood estimator:</p>
<p><span class="math display">\[\begin{align*}
  &amp;\hat{\Sigma}_{MLE} = \arg\max_{\Sigma \in \mathbb{S}_{+}^{p}}\left\{ const. - \frac{n}{2}\log\left| \Sigma \right| - tr\left[ \frac{1}{2}\sum_{i = 1}^{n}\left(X_{i} - \mu \right)\left(X_{i} - \mu \right)^{T}\Sigma^{-1} \right] \right\} \\
  &amp;\nabla_{\Sigma}l(\mu, \Sigma | X) = -\frac{n}{2}\Sigma^{-1} + \frac{1}{2}\sum_{i = 1}^{n}\left(X_{i} - \mu \right)\left(X_{i} - \mu \right)^{T}\Sigma^{-2} \\
  \Rightarrow &amp;\hat{\Sigma}_{MLE} = \left[ \frac{1}{n}\sum_{i = 1}^{n}\left(X_{i} - \bar{X} \right)\left(X_{i} - \bar{X} \right)^{T} \right]
\end{align*}\]</span></p>
<p>By setting the gradient equal to zero and plugging in the MLE for <span class="math inline">\(\mu\)</span>, we find that the MLE for <span class="math inline">\(\Sigma\)</span> is our usual sample estimator often denoted as <span class="math inline">\(S\)</span>. It turns out that we could have just as easily computed the maximum likelihood estimator for the precision matrix <span class="math inline">\(\Omega \equiv \Sigma^{-1}\)</span> and taken its inverse:</p>
<p><span class="math display">\[ \hat{\Omega}_{MLE} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega\right| \right\} \]</span></p>
<p>so that <span class="math inline">\(\hat{\Omega}_{MLE} = S^{-1}\)</span>. Beyond the formatting convenience, computing estimates for <span class="math inline">\(\Omega\)</span> as opposed to <span class="math inline">\(\Sigma\)</span> often poses less computational challenges – and accordingly, the literature has placed more emphasis on efficiently solving for <span class="math inline">\(\Omega\)</span> instead of <span class="math inline">\(\Sigma\)</span>.</p>
<p>As in regression settings, we can construct a <em>penalized</em> log-likelihood estimator by adding a penalty term, <span class="math inline">\(P\left(\Omega\right)\)</span>, to the likelihood:</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega \right| + P\left( \Omega \right) \right\} \]</span></p>
<p><span class="math inline">\(P\left( \Omega \right)\)</span> is often of the form <span class="math inline">\(P\left(\Omega \right) = \lambda\|\Omega \|_{F}^{2}/2\)</span> or <span class="math inline">\(P\left(\Omega \right) = \|\Omega\|_{1}\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span>, <span class="math inline">\(\left\|\cdot \right\|_{F}^{2}\)</span> is the Frobenius norm and we define <span class="math inline">\(\left\|A \right\|_{1} = \sum_{i, j} \left| A_{ij} \right|\)</span>. These penalties are the ridge and lasso, respectively. The penalty proposed by <span class="citation">Molstad and Rothman (2017)</span> is one of the following form:</p>
<p><span class="math display">\[ P\left(\Omega\right) = \lambda\left\| A\Omega B - C \right\|_{1} \]</span></p>
<p>where <span class="math inline">\(A \in \mathbb{R}^{m \times p}, B \in \mathbb{R}^{p \times q}, \mbox{ and } C \in \mathbb{R}^{m \times q}\)</span> are matrices assumed to be known and specified by the user. Solving the full penalized log-likelihood for <span class="math inline">\(\Omega\)</span> results in solving</p>
<p><span class="math display">\[ \hat{\Omega} = \arg\min_{\Omega \in S_{+}^{p}}\left\{ tr\left(S\Omega\right) - \log\left|\Omega \right| + \lambda\left\| A\Omega B - C \right\|_{1} \right\} \]</span></p>
<p>This form of penalty is particularly useful because matrices <span class="math inline">\(A, B, \mbox{ and } C\)</span> can be constructed so that we penalize the sum, absolute value of a <em>characteristic</em> of the precision matrix <span class="math inline">\(\Omega\)</span>. This type of penalty leads to many new, interesting, and novel estimators for <span class="math inline">\(\Omega\)</span>. An example of one such estimator (suppose we observe <span class="math inline">\(n\)</span> samples of <span class="math inline">\(Y_{i} \in \mathbb{R}^{r}\)</span>) would be one where we set <span class="math inline">\(A = I_{p}, B = \Sigma_{xy}, \mbox{ and } C = 0\)</span> where <span class="math inline">\(\Sigma_{xy}\)</span> is the covariance matrix of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This penalty has the effect of assuming sparsity in the forward regression coefficient <span class="math inline">\(\beta \equiv \Omega\Sigma_{xy}\)</span>. Of course, in practice we do not know the true covariance matrix <span class="math inline">\(\Sigma_{xy}\)</span> but we might consider using the sample estimate <span class="math inline">\(\hat{\Sigma}_{xy} = \sum_{i = 1}^{n}\left(X_{i} - \bar{X}\right)\left(Y_{i} - \bar{Y}\right)^{T}/n\)</span></p>
<p>We will explore how to solve for <span class="math inline">\(\hat{\Omega}\)</span> in the next section.</p>
<p><br></p>
</div>
<div id="augmented-admm-algorithm" class="section level2">
<h2 class="hasAnchor">
<a href="#augmented-admm-algorithm" class="anchor"></a>Augmented ADMM Algorithm</h2>
<p><em>This section requires general knowledge of the alternating direction method of multipliers (ADMM) algorithm. I would recommend reading this overview I have written <a href="https://mgallow.github.io/ADMMsigma/articles/Details.html#admm-algorithm">here</a> before proceeding.</em></p>
<p>The ADMM algorithm - thanks to it’s flexibility - is particularly well-suited to solve penalized-likelihood optimization problems that arise naturally in several statistics and machine learning applications. Within the context of <span class="citation">Molstad and Rothman (2017)</span>, this algorithm would consist of iterating over the following three steps:</p>
<p><span class="math display">\[\begin{align}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}L_{\rho}(\Omega, Z^{k}, \Lambda^{k}) \\
  Z^{k + 1} &amp;= \arg\min_{Z \in \mathbb{R}^{n \times r}}L_{\rho}(\Omega^{k + 1}, Z, \Lambda^{k}) \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
\end{align}\]</span></p>
<p>where <span class="math inline">\(L_{p}(\cdot)\)</span> is the <em>augmented lagrangian</em> defined as</p>
<p><span class="math display">\[ L_{\rho}(\Omega, Z, \Lambda) = f\left(\Omega\right) + g\left(Z\right) + tr\left[\Lambda^{T}\left(A\Omega B - Z - C\right)\right] + \frac{\rho}{2}\left\|A\Omega B - Z - C\right\|_{F}^{2} \]</span></p>
<p>with <span class="math inline">\(f\left(\Omega\right) = tr\left(S\Omega\right) - \log\left|\Omega\right|\)</span> and <span class="math inline">\(g\left(Z\right) = \lambda\left\|Z\right\|_{1}\)</span>. However, instead of solving the first step exactly, the authors propose an alternative, approximating objective function (<span class="math inline">\(\tilde{L}\)</span>) based on the majorize-minimize principle – the purpose of which is to find a solution that can be solved in closed form.</p>
<p>The approximating function is defined as</p>
<p><span class="math display">\[\begin{align*}
  \tilde{L}_{\rho}\left(\Omega, Z^{k}, \Lambda^{k}\right) = f\left(\Omega\right) &amp;+ tr\left[(\Lambda^{k})^{T}(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \\
  &amp;+ \frac{\rho}{2}vec\left(\Omega - \Omega^{k}\right)^{T}Q\left(\Omega - \Omega^{k}\right)
\end{align*}\]</span></p>
<p>where <span class="math inline">\(Q = \tau I_{p} - \left(A^{T}A \otimes BB^{T}\right)\)</span>, <span class="math inline">\(\otimes\)</span> is the Kronecker product, and <span class="math inline">\(\tau\)</span> is chosen such that <span class="math inline">\(Q\)</span> is positive definite. Note that if <span class="math inline">\(Q\)</span> is positive definite (p.d.), then</p>
<p><span class="math display">\[ \frac{\rho}{2}vec\left(\Omega - \Omega^{k} \right)^{T}Q\left(\Omega - \Omega^{k} \right) &gt; 0 \]</span></p>
<p>since <span class="math inline">\(\rho &gt; 0\)</span> and <span class="math inline">\(vec\left(\Omega - \Omega^{k}\right)\)</span> is always nonzero whenever <span class="math inline">\(\Omega \neq \Omega^{k}\)</span>. Thus <span class="math inline">\(L_{\rho}\left(\cdot\right) \leq \tilde{L}\left(\cdot\right)\)</span> for all <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\tilde{L}\)</span> is a majorizing function.</p>
<p>To see why this particular function was used, consider the Taylor’s expansion of <span class="math inline">\(\rho\left\|A\Omega B - Z^{k} - C\right\|_{F}^{2}/2\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \frac{\rho}{2}\left\| A\Omega B - Z^{k} - C \right\|_{F}^{2} &amp;\approx \frac{\rho}{2}\left\| A\Omega^{k} B - Z^{k} - C \right\|_{F}^{2} \\
  &amp;+ \frac{\rho}{2}vec\left( \Omega - \Omega^{k}\right)^{T}\left(A^{T}A \otimes BB^{T}\right)vec\left(\Omega - \Omega^{k}\right) \\
  &amp;+ \rho vec\left(\Omega - \Omega^{k}\right)^{T}vec\left(BB^{T}\Omega^{k}A^{T}A - B(Z^{k})^{T}A - BC^{T}A \right)
\end{align*}\]</span></p>
<p><strong>Note:</strong></p>
<p><span class="math display">\[\begin{align*}
  &amp;\nabla_{\Omega}\left\{ \frac{\rho}{2}\left\|A\Omega B - Z - C\right\|_{F}^{2} \right\} = \rho BB^{T}\Omega A^{T}A - \rho BZ^{T}A - \rho BC^{T}A \\
  &amp;\nabla_{\Omega}^{2}\left\{ \frac{\rho}{2}\left\|A\Omega B - Z - C \right\|_{F}^{2} \right\} = \rho\left(A^{T}A \otimes BB^{T} \right)
\end{align*}\]</span></p>
<p>This implies that</p>
<p><span class="math display">\[\begin{align*}
  \frac{\rho}{2}\left\| A\Omega B - Z^{k} - C \right\|_{F}^{2} &amp;+ \frac{\rho}{2}vec\left(\Omega - \Omega^{k} \right)^{T}Q\left(\Omega - \Omega^{k} \right) \\
  &amp;\approx \frac{\rho}{2}\left\| A\Omega^{k} B - Z^{k} - C \right\|_{F}^{2} + \frac{\rho}{2}vec\left(\Omega - \Omega^{k} \right)^{T}Q\left(\Omega - \Omega^{k} \right) \\
  &amp;+ \frac{\rho}{2}vec\left( \Omega - \Omega^{k}\right)^{T}\left(A^{T}A \otimes BB^{T}\right)vec\left(\Omega - \Omega^{k}\right) \\
  &amp;+ \rho vec\left(\Omega - \Omega^{k}\right)^{T}vec\left(BB^{T}\Omega^{k}A^{T}A - B(Z^{k})^{T}A - BC^{T}A \right) \\
  &amp;= \frac{\rho}{2}\left\| A\Omega^{k} B - Z^{k} - C \right\|_{F}^{2} + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \\
  &amp;+ \rho tr\left[\left(\Omega - \Omega^{k}\right)\left(BB^{T}\Omega^{k}A^{T}A - B(Z^{k})^{T}A - BC^{T}A \right)\right]
\end{align*}\]</span></p>
<p>Let us now plug in this equality into our optimization problem in step one:</p>
<p><span class="math display">\[\begin{align*}
  \Omega^{k + 1} &amp;:= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\tilde{L}_{\rho}(\Omega, Z^{k}, \Lambda^{k}) \\
  &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{\begin{matrix}
 tr\left(S\Omega\right) - \log\left|\Omega\right| + tr\left[(\Lambda^{k})^{T}(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \end{matrix}\right. \\
  &amp;+ \left.\begin{matrix} \frac{\rho}{2}vec\left(\Omega - \Omega^{k}\right)^{T}Q\left(\Omega - \Omega^{k}\right) \end{matrix}\right\} \\
  &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{\begin{matrix}
 tr\left(S\Omega\right) - \log\left|\Omega\right| + tr\left[(\Lambda^{k})^{T}(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega^{k} B - Z^{k} - C \right\|_{F}^{2} \end{matrix}\right. \\
  &amp;+ \left.\begin{matrix} \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} + \rho tr\left[\left(\Omega - \Omega^{k}\right)\left(BB^{T}\Omega^{k}A^{T}A - B(Z^{k})^{T}A - BC^{T}A \right)\right] \end{matrix}\right\} \\
  &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{\begin{matrix}
 tr\left[\left(S + \rho A^{T}(A\Omega^{k}B - Z^{k} - C + \Lambda^{k}/\rho)B^{T} \right)\Omega\right] \end{matrix}\right. \\
  &amp;- \left.\begin{matrix} \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \end{matrix}\right\} \\
  &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{
 tr\left[\left(S + G^{k} \right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\} \\
\end{align*}\]</span></p>
<p>where <span class="math inline">\(G^{k} = \rho A^{T}(A\Omega^{k}B - Z^{k} - C + \Lambda^{k}/\rho)B^{T}\)</span>.</p>
<p><br></p>
<p>The <em>augmented ADMM</em> algorithm is the following:</p>
<p><span class="math display">\[\begin{align}
  \Omega^{k + 1} &amp;= \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{tr\left[\left(S + G^{k}\right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\} \\
  Z^{k + 1} &amp;= \arg\min_{Z \in \mathbb{R}^{n \times r}}\left\{\lambda\left\|Z\right\|_{1} + tr\left[(\Lambda^{k})^{T}(A\Omega B - Z^{k} - C) \right] + \frac{\rho}{2}\left\|A\Omega B - Z^{k} - C \right\|_{F}^{2} \right\} \\
  \Lambda^{k + 1} &amp;= \Lambda^{k} + \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right)
\end{align}\]</span></p>
<p><br></p>
<div id="algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#algorithm" class="anchor"></a>Algorithm</h3>
<p>Set <span class="math inline">\(k = 0\)</span> and repeat steps 1-6 until convergence.</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\(G^{k} = \rho A^{T}\left( A\Omega^{k} B - Z^{k} - C + \rho^{-1}Y^{k} \right)B^{T}\)</span></p></li>
<li><p>Decompose <span class="math inline">\(S + \left( G^{k} + (G^{k})^{T} \right)/2 - \rho\tau\Omega^{k} = VQV^{T}\)</span> (via the spectral decomposition).</p></li>
<li><p>Set <span class="math inline">\(\Omega^{k + 1} = V\left( -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2} \right)V^{T}/(2\rho\tau)\)</span></p></li>
<li><p>Set <span class="math inline">\(Z^{k + 1} = \mbox{soft}\left( A\Omega^{k + 1}B - C + \rho^{-1}Y^{k}, \rho^{-1}\lambda \right)\)</span></p></li>
<li><p>Set <span class="math inline">\(Y^{k + 1} = \rho\left( A\Omega^{k + 1} B - Z^{k + 1} - C \right)\)</span></p></li>
<li><p>Replace <span class="math inline">\(k\)</span> with <span class="math inline">\(k + 1\)</span>.</p></li>
</ol>
<p>where <span class="math inline">\(\mbox{soft}(a, b) = \mbox{sign}(a)(\left| a \right| - b)_{+}\)</span>.</p>
<p><br></p>
</div>
<div id="proof-of-2-3" class="section level3">
<h3 class="hasAnchor">
<a href="#proof-of-2-3" class="anchor"></a>Proof of (2-3):</h3>
<p><span class="math display">\[ \Omega^{k + 1} = \arg\min_{\Omega \in \mathbb{S}_{+}^{p}}\left\{tr\left[\left(S + G^{k}\right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\} \]</span></p>
<p><span class="math display">\[\begin{align*}
  &amp;\nabla_{\Omega}\left\{tr\left[\left(S + G^{k}\right)\Omega\right] - \log\left|\Omega\right| + \frac{\rho\tau}{2}\left\|\Omega - \Omega^{k}\right\|_{F}^{2} \right\} \\
  &amp;= 2S - S\circ I_{p} + G^{k} + (G^{k})^{T} - G^{k}\circ I_{p} - 2\Omega^{-1} + \Omega^{-1}\circ I_{p} \\
  &amp;+ \frac{\rho\tau}{2}\left[2\Omega - 2(\Omega^{k})^{T} + 2\Omega^{T} - 2\Omega^{k} - 2(\Omega - \Omega^{k})^{T}\circ I_{p} \right]
\end{align*}\]</span></p>
<p>Note that we need to honor the symmetric constraint given by <span class="math inline">\(\Omega\)</span>. By setting the gradient equal to zero and multiplying all off-diagonal elements by <span class="math inline">\(1/2\)</span>, this simplifies to</p>
<p><span class="math display">\[ S + \frac{1}{2}\left(G^{k} + (G^{k})^{T}\right) - \rho\tau\Omega^{k} = (\Omega^{k + 1})^{-1} - \rho\tau\Omega^{k + 1} \]</span></p>
<p>We can then decompose <span class="math inline">\(\Omega^{k + 1} = VDV^{T}\)</span> where <span class="math inline">\(D\)</span> is a diagonal matrix with diagonal elements equal to the eigen values of <span class="math inline">\(\Omega^{k + 1}\)</span> and <span class="math inline">\(V\)</span> is the matrix with corresponding eigen vectors as columns.</p>
<p><span class="math display">\[ S + \frac{1}{2}\left(G^{k} + (G^{k})^{T}\right) - \rho\tau\Omega^{k} = VD^{-1}V^{T} - \rho\tau VDV^{T} = V\left(D^{-1} - \rho\tau D\right)V^{T} \]</span></p>
<p>This equivalence implies that</p>
<p><span class="math display">\[ \phi_{j}\left( D^{k} \right) = \frac{1}{\phi_{j}(\Omega^{k + 1})} - \rho\tau\phi_{j}(\Omega^{k + 1}) \]</span></p>
<p>where <span class="math inline">\(\phi_{j}(\cdot)\)</span> is the <span class="math inline">\(j\)</span>th eigen value and <span class="math inline">\(D^{k} = S + \left(G^{k} + (G^{k})^{T}\right)/2 - \rho\tau\Omega^{k}\)</span>. Therefore</p>
<p><span class="math display">\[\begin{align*}
  &amp;\Rightarrow \rho\tau\phi_{j}^{2}(\Omega^{k + 1}) + \phi_{j}\left( D^{k} \right)\phi_{j}(\Omega^{k + 1}) - 1 = 0 \\
  &amp;\Rightarrow \phi_{j}(\Omega^{k + 1}) = \frac{-\phi_{j}(D^{k}) \pm \sqrt{\phi_{j}^{2}(D^{k}) + 4\rho\tau}}{2\rho\tau}
\end{align*}\]</span></p>
<p>In summary, if we decompose <span class="math inline">\(S + \left(G^{k} + (G^{k})^{T}\right)/2 - \rho\tau\Omega^{k} = VQV^{T}\)</span> then</p>
<p><span class="math display">\[ \Omega^{k + 1} = \frac{1}{2\rho\tau}V\left[ -Q + (Q^{2} + 4\rho\tau I_{p})^{1/2}\right] V^{T} \]</span></p>
<p><br></p>
</div>
<div id="proof-of-4" class="section level3">
<h3 class="hasAnchor">
<a href="#proof-of-4" class="anchor"></a>Proof of (4)</h3>
<p><span class="math display">\[ Z^{k + 1} = \arg\min_{Z \in \mathbb{R}^{n \times r}}\left\{ \lambda\left\| Z \right\|_{1} + tr\left[(\Lambda^{k})^{T}\left(A\Omega^{k + 1}B - Z - C\right)\right] + \frac{\rho}{2}\left\| A\Omega^{k + 1}B - Z - C \right\|_{F}^{2} \right\} \]</span></p>

<p><span class="math display">\[\begin{align*}
  \partial&amp;\left\{ \lambda\left\| Z \right\|_{1} + tr\left[(\Lambda^{k})^{T}\left(A\Omega^{k + 1}B - Z - C\right)\right] + \frac{\rho}{2}\left\| A\Omega^{k + 1}B - Z - C \right\|_{F}^{2} \right\} \\
  &amp;= \partial\left\{ \lambda\left\| Z \right\|_{1} \right\} + \nabla_{\Omega}\left\{ tr\left[(\Lambda^{k})^{T}\left(A\Omega^{k + 1}B - Z - C\right)\right] + \frac{\rho}{2}\left\| A\Omega^{k + 1}B - Z - C \right\|_{F}^{2} \right\} \\
  &amp;= \mbox{sign}(Z)\lambda - \Lambda^{k} - \rho\left( A\Omega^{k + 1}B - Z - C \right)
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\mbox{sign(Z)}\)</span> is the elementwise sign operator. By setting the gradient/sub-differential equal to zero, we arrive at the following equivalence:</p>
<p><span class="math display">\[ Z_{ij}^{k + 1} = \frac{1}{\rho}\left( \rho(A\Omega_{ij}^{k + 1}B - C) + \Lambda_{ij}^{k} - Sign(Z_{ij}^{k + 1})\lambda \right) \]</span></p>
<p>for all <span class="math inline">\(i = 1,..., p\)</span> and <span class="math inline">\(j = 1,..., p\)</span>. We observe two scenarios:</p>
<ul>
<li>If <span class="math inline">\(Z_{ij}^{k + 1} &gt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\left(A\Omega_{ij}^{k + 1}B - C\right) + \Lambda_{ij}^{k} &gt; \lambda\alpha \]</span></p>
<ul>
<li>If <span class="math inline">\(Z_{ij}^{k + 1} &lt; 0\)</span> then</li>
</ul>
<p><span class="math display">\[ \rho\left(A\Omega_{ij}^{k + 1}B - C\right) + \Lambda_{ij}^{k} &lt; -\lambda\alpha \]</span></p>
<p>This implies that <span class="math inline">\(\mbox{sign}(Z_{ij}^{k + 1}) = \mbox{sign}\left(\rho(A\Omega_{ij}^{k + 1}B - C) + \Lambda_{ij}^{k}\right)\)</span>. Putting all the pieces together, we arrive at</p>
<p><span class="math display">\[\begin{align*}
Z_{ij}^{k + 1} &amp;= \frac{1}{\rho}\mbox{sign}\left(\rho(A\Omega_{ij}^{k + 1}B - C) + \Lambda_{ij}^{k}\right)\left( \left| \rho(A\Omega_{ij}^{k + 1}B - C) + \Lambda_{ij}^{k} \right| - \lambda \right)_{+} \\
&amp;= \frac{1}{\rho}\mbox{soft}\left(\rho(A\Omega_{ij}^{k + 1}B - C) + \Lambda_{ij}^{k}, \lambda\right)
\end{align*}\]</span></p>
<p>where soft is the soft-thresholding function.</p>
<p><br></p>
</div>
</div>
<div id="stopping-criterion" class="section level2">
<h2 class="hasAnchor">
<a href="#stopping-criterion" class="anchor"></a>Stopping Criterion</h2>
<p>In discussing the optimality conditions and stopping criterion, we will follow the steps outlined in <span class="citation">Boyd et al. (2011)</span> and cater them to the SCPME method.</p>
<p>Below we have three optimality conditions:</p>
<ol style="list-style-type: decimal">
<li>Primal:</li>
</ol>
<p><span class="math display">\[ A\Omega^{k + 1}B - Z^{k + 1} - C = 0 \]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Dual:</li>
</ol>
<p><span class="math display">\[\begin{align*}
  0 &amp;\in \partial f\left(\Omega^{k + 1}\right) + \frac{1}{2}\left(B(\Lambda^{k + 1})^{T}A + A^{T}\Lambda^{k + 1}B^{T} \right) \\
  0 &amp;\in \partial g\left(Z^{k + 1}\right) - \Lambda^{k + 1}
\end{align*}\]</span></p>
<p>The first dual optimality condition is a result of taking the sub-differential of the lagrangian (non-augmented) with respect to <span class="math inline">\(\Omega^{k + 1}\)</span> (note that we must honor the symmetric constraint of <span class="math inline">\(\Omega^{k + 1}\)</span>) and the second is a result of taking the sub-differential of the lagrangian with respect to <span class="math inline">\(Z^{k + 1}\)</span> (no symmetric constraint).</p>
<p>We will define the left-hand side of the primal optimality condition as the primal residual <span class="math inline">\(r^{k + 1} = A\Omega^{k + 1}B - Z^{k + 1} - C\)</span>. At convergence, the optimality conditions require that <span class="math inline">\(r^{k + 1} \approx 0\)</span>. The second residual we will define is the dual residual:</p>
<p><span class="math display">\[ s^{k + 1} = \frac{\rho}{2}\left( B(Z^{k + 1} - Z^{k})^{T}A + A^{T}(Z^{k + 1} - Z^{k})B^{T} \right) \]</span></p>
<p>This residual is derived from the following:</p>
<p>Because <span class="math inline">\(\Omega^{k + 1}\)</span> is the argument that minimizes <span class="math inline">\(L_{p}\left( \Omega, Z^{k}, \Lambda^{k} \right)\)</span>,</p>
<p><span class="math display">\[\begin{align*}
  0 &amp;\in \partial \left\{ f\left(\Omega^{k + 1}\right) + tr\left[ \Lambda^{k}\left( A\Omega^{k + 1}B - Z^{k} - C \right) \right] + \frac{\rho}{2}\left\| A\Omega^{k + 1}B - Z^{k} - C \right\|_{F}^{2} \right\} \\
  &amp;= \partial f\left(\Omega^{k + 1} \right) + \frac{1}{2}\left(B(\Lambda^{k})^{T}A + A^{T}\Lambda^{k}B^{T} \right) + \frac{\rho}{2}\left( BB^{T}\Omega^{k + 1}A^{T}A + A^{T}A\Omega^{k + 1}BB^{T} \right) \\
  &amp;- \frac{\rho}{2}\left( A^{T}(Z^{k} + C)B^{T} + B(Z^{k} + C)^{T}A \right) \\
  &amp;= \partial f\left(\Omega^{k + 1} \right) + \frac{1}{2}\left(B(\Lambda^{k})^{T}A + A^{T}\Lambda^{k}B^{T} \right) \\
  &amp;+ \frac{\rho}{2}\left( B(B^{T}\Omega^{k + 1}A^{T} - (Z^{k})^{T} - C^{T})A + A^{T}(A\Omega^{k + 1}B - Z^{k} - C)B^{T} \right) \\
  &amp;= \partial f\left(\Omega^{k + 1} \right) + \frac{1}{2}\left( B(\Lambda^{k})^{T}A + A^{T}\Lambda^{k}B^{T} \right) + \frac{\rho}{2}\left(A^{T}(A\Omega^{k + 1}B - Z^{k + 1} + Z^{k + 1} - Z^{k} - C)B^{T} \right) \\
  &amp;+ \frac{\rho}{2}\left(B(B^{T}\Omega^{k + 1}A^{T} - (Z^{k + 1})^{T} + (Z^{k + 1})^{T} - (Z^{k})^{T} - C^{T})A \right) \\
  &amp;= \partial f\left(\Omega^{k + 1} \right) + \frac{1}{2}\left[ B\left((\Lambda^{k})^{T} + \rho(B^{T}\Omega^{k + 1}A^{T} - (Z^{k + 1})^{T} - C^{T}) \right)A \right] \\
  &amp;+ \frac{1}{2}\left[ A^{T}\left(\Lambda^{k} + \rho(A\Omega^{k + 1}B - Z^{k + 1} - c)B \right)B^{T} \right] + \frac{\rho}{2}\left(B(Z^{k + 1} - Z^{k})^{T}A + A^{T}(Z^{k + 1} - Z^{k})B^{T} \right) \\
  &amp;= \partial f\left(\Omega^{k + 1} \right) + \frac{1}{2}\left(B(\Lambda^{k + 1})^{T}A + A^{T}\Lambda^{k + 1}B^{T} \right) + \frac{\rho}{2}\left(B(Z^{k + 1} - Z^{k})^{T}A + A^{T}(Z^{k + 1} - Z^{k})B^{T} \right) \\
  \Rightarrow 0 &amp;\in \frac{\rho}{2}\left( B(Z^{k + 1} - Z^{k})^{T}A + A^{T}(Z^{k + 1} - Z^{k})B^{T} \right)
\end{align*}\]</span></p>
<p>Like the primal residual, at convergence the optimality conditions require that <span class="math inline">\(s^{k + 1} \approx 0\)</span>. Note that the second dual optimality condition is always satisfied:</p>
<p><span class="math display">\[\begin{align*}
  0 &amp;\in \partial \left\{ g\left(Z^{k + 1}\right) + tr\left[ \Lambda^{k}\left( A\Omega^{k + 1}B - Z^{k + 1} - C \right) \right] + \rho\left\| A\Omega^{k + 1}B - Z^{k + 1} - C \right\|_{F}^{2} \right\} \\
  &amp;= \partial g\left(Z^{k + 1}\right) - \Lambda^{k} - \rho\left(A\Omega^{k + 1}B - Z^{k + 1} - C \right) \\
  &amp;= \partial g\left(Z^{k + 1}\right) - \Lambda^{k + 1} \\
\end{align*}\]</span></p>
<p>One possible stopping criterion is to set <span class="math inline">\(\epsilon^{rel} = \epsilon^{abs} = 10^{-3}\)</span> and stop the algorithm when <span class="math inline">\(\epsilon^{pri} \leq \left\| r^{k + 1} \right\|_{F}\)</span> and <span class="math inline">\(\epsilon^{dual} \leq \left\| s^{k + 1} \right\|_{F}\)</span> where</p>
<p><span class="math display">\[\begin{align*}
  \epsilon^{pri} &amp;= \sqrt{nr}\epsilon^{abs} + \epsilon^{rel}\max\left\{ \left\| A\Omega^{k + 1}B \right\|_{F}, \left\| Z^{k + 1} \right\|_{F}, \left\| C \right\|_{F} \right\} \\
  \epsilon^{dual} &amp;= p\epsilon^{abs} + \epsilon^{rel}\left\| \left( B(\Lambda^{k + 1})^{T}A + A^{T}\Lambda^{k + 1}B^{T} \right)/2 \right\|_{F}
\end{align*}\]</span></p>
<p><br><br></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-boyd2011distributed">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, and others. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1). Now Publishers, Inc.: 1–122.</p>
</div>
<div id="ref-molstad2017shrinking">
<p>Molstad, Aaron J, and Adam J Rothman. 2017. “Shrinking Characteristics of Precision Matrix Estimators.” <em>Biometrika</em>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#introduction">Introduction</a></li>
      <li><a href="#augmented-admm-algorithm">Augmented ADMM Algorithm</a></li>
      <li><a href="#stopping-criterion">Stopping Criterion</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Matt Galloway.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
